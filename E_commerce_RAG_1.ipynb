{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFdftzySj6SR",
        "outputId": "e6ea9776-2d34-4cf0-ded0-6204e1331792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Change directory to the folder containing your notebook or dataset\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4sqX1b2kkL3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load files\n",
        "rfm = pd.read_csv(\"rfm.xls\")\n",
        "context_recon = pd.read_csv(\"contextual_policy_recommendations.xls\")\n",
        "context_policy = pd.read_csv(\"contextual_policy_summary.xls\")\n",
        "context_action = pd.read_csv(\"contextual_policy_tier_action_mix.xls\")\n",
        "retail = pd.read_csv(\"Online_retail_cleaned.xls\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i44mUT1Mg5YL"
      },
      "source": [
        "Google API key Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJCcehw6kkJS",
        "outputId": "9211f744-d64d-4f99-bf03-aa0445e24a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your Google API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "API key successfully loaded for this session.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Try to load the API key from a .env file.\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# If the key is not found, prompt the user to enter it.\n",
        "if not api_key:\n",
        "    from getpass import getpass\n",
        "    api_key = getpass(\"Please enter your Google API key: \")\n",
        "    os.environ['GOOGLE_API_KEY'] = api_key\n",
        "\n",
        "    # Check if the key was entered\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not entered. Please provide your key.\")\n",
        "\n",
        "# You can now proceed with initializing the client\n",
        "# The `api_key` variable is now guaranteed to exist for this session.\n",
        "print(\"API key successfully loaded for this session.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiP3gx9Ng8wY"
      },
      "source": [
        "**Generating Natural Language Customer RFM Summaries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REmhYNBlhBOf"
      },
      "source": [
        "This code snippet iterates through a pandas DataFrame named rfm, which presumably contains the results of a Recency, Frequency, and Monetary (RFM) analysis. For every customer record, it extracts their RFM metrics and their churn status (from the Churn_Label column). It then uses an f-string to generate a descriptive, human-readable text summary of the customer's behavior and classification. These summaries are collected into a list called rfm_docs, effectively converting structured numerical data into a list of natural language documents, before printing the first five for review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0pq2ZQikkGp",
        "outputId": "2753eea8-763a-40d2-f9ee-c00e508a5427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Customer 12346 has a recency of 66 days, a purchase frequency of 14 times, and a total monetary spend of $372.86. This customer is classified as churned.', 'Customer 12347 has a recency of 2 days, a purchase frequency of 2 times, and a total monetary spend of $1323.32. This customer is classified as not churned.', 'Customer 12348 has a recency of 73 days, a purchase frequency of 1 times, and a total monetary spend of $222.16. This customer is classified as churned.', 'Customer 12349 has a recency of 42 days, a purchase frequency of 3 times, and a total monetary spend of $2064.39. This customer is classified as churned.', 'Customer 12351 has a recency of 10 days, a purchase frequency of 1 times, and a total monetary spend of $300.93. This customer is classified as not churned.']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "rfm_docs = []\n",
        "for index, row in rfm.iterrows():\n",
        "    customer_id = row['Customer ID']\n",
        "    recency = row['Recency']\n",
        "    frequency = row['Frequency']\n",
        "    monetary = row['Monetary']\n",
        "    churn = 'churned' if row['Churn_Label'] == 1 else 'not churned'\n",
        "\n",
        "    text = f\"Customer {customer_id} has a recency of {recency} days, a purchase frequency of {frequency} times, and a total monetary spend of ${monetary:.2f}. This customer is classified as {churn}.\"\n",
        "    rfm_docs.append(text)\n",
        "\n",
        "# You can now see the first few documents\n",
        "print(rfm_docs[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6ALzjvHhEg2"
      },
      "source": [
        "**Generating Contextual Policy Recommendation Summaries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik5__2evhEZB"
      },
      "source": [
        "This code snippet processes a DataFrame named context_recon (which contains contextual policy recommendations). It iterates over each customer record to extract the Customer_ID, the Chosen_Action (the recommended retention strategy), and the Estimated_Reward (the projected financial return, or ROI) for that action. It then converts this structured data into a descriptive text format, creating a sentence that summarizes the recommended action and its predicted ROI for each specific customer. Finally, these text summaries are collected in a list called policy_docs, with the first five documents being printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AST0ByHhkkEP",
        "outputId": "f310874c-678e-4d8d-9a4c-aea4f4fd453b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"For customer 12346, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $2.15.\", \"For customer 12347, the recommended retention action is to use 'email'. This action has a projected ROI of $0.00.\", \"For customer 12348, the recommended retention action is to use 'none'. This action has a projected ROI of $0.00.\", \"For customer 12349, the recommended retention action is to use 'sms'. This action has a projected ROI of $2.99.\", \"For customer 12351, the recommended retention action is to use 'sms+coupon'. This action has a projected ROI of $0.00.\"]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you've already loaded the dataframe\n",
        "# context_recon = pd.read_csv(\"contextual_policy_recommendations.csv\")\n",
        "\n",
        "policy_docs = []\n",
        "for index, row in context_recon.iterrows():\n",
        "    customer_id = row['Customer_ID']\n",
        "    action = row['Chosen_Action']\n",
        "    reward = row['Estimated_Reward']\n",
        "\n",
        "    text = f\"For customer {customer_id}, the recommended retention action is to use '{action}'. This action has a projected ROI of ${reward:.2f}.\"\n",
        "    policy_docs.append(text)\n",
        "\n",
        "print(policy_docs[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GqVDbbOhM7U"
      },
      "source": [
        "**Generating Contextual Policy Action Summaries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS2ql3W2hQ5d"
      },
      "source": [
        "This code snippet aggregates a summary of recommended actions from a DataFrame named context_policy. It iterates through each row, extracting the specific retention action (Chosen_Action), the average projected return on investment (Average_Reward), and the volume (number of customers) assigned to that action. The code then compiles this information into a list of descriptive text documents (summary_docs), which clearly state how many customers were given a particular action and what the action's overall average projected financial benefit is, before printing the resulting summary documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmBuquWYkkBq",
        "outputId": "cffa29e4-4877-4187-d9e0-1de8066a4cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The retention action 'call+coupon' was assigned to 2247 customers, with an average projected ROI of $3.74.\", \"The retention action 'sms+coupon' was assigned to 803 customers, with an average projected ROI of $2.13.\", \"The retention action 'sms' was assigned to 359 customers, with an average projected ROI of $0.05.\", \"The retention action 'email' was assigned to 462 customers, with an average projected ROI of $0.00.\", \"The retention action 'none' was assigned to 418 customers, with an average projected ROI of $0.00.\"]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you've already loaded the dataframe\n",
        "# context_policy = pd.read_csv(\"contextual_policy_summary.csv\")\n",
        "\n",
        "summary_docs = []\n",
        "for index, row in context_policy.iterrows():\n",
        "    action = row['Chosen_Action']\n",
        "    avg_reward = row['Average_Reward']\n",
        "    volume = row['Customers']\n",
        "\n",
        "    text = f\"The retention action '{action}' was assigned to {volume} customers, with an average projected ROI of ${avg_reward:.2f}.\"\n",
        "    summary_docs.append(text)\n",
        "\n",
        "print(summary_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tLfgkGDhUGN"
      },
      "source": [
        "**Extracting All Textual Content**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuqoDbuMhXv1"
      },
      "source": [
        "This code snippet's purpose is to read and extract all the textual content‚Äîboth markdown and code‚Äîfrom a Jupyter Notebook file. It first uses the json library to load the raw structure of the notebook specified by notebook_path. It then iterates through every cell in the notebook, checks if the cell contains source code or text (in the source field), joins all the lines from that cell, and concatenates them into a single, continuous string named notebook_text. This effectively flattens the notebook's content into a searchable text document, with the final line printing the first 500 characters to verify the extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uksp5B1akj-G",
        "outputId": "0fb1d7db-0830-4369-b7de-bdc62168a8e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Loading Dataset & Info**\n",
            "\n",
            "This Python code snippet defines a function, process_online_retail_data, that downloads, cleans, and transforms the \"Online Retail\" dataset from a UCI Machine Learning repository URL. It first uses the requests library to fetch the Excel file and pandas to load it into a DataFrame. The function then performs extensive data cleaning‚Äîincluding dropping missing CustomerID values, removing duplicates, filtering out non-positive unit prices, and handling outliers based on \n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "notebook_path = \"E-commerce_1_1.ipynb\"\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    notebook_content = json.load(f)\n",
        "\n",
        "# Concatenate all text and code cells into a single string\n",
        "notebook_text = \"\"\n",
        "for cell in notebook_content['cells']:\n",
        "    if 'source' in cell and isinstance(cell['source'], list):\n",
        "        notebook_text += \"\".join(cell['source']) + \"\\n\\n\"\n",
        "\n",
        "# You can now print the first 500 characters to verify\n",
        "print(notebook_text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIZHjnn2kj7V",
        "outputId": "1c5392c1-4d62-44b6-b79c-5aa2f62c3e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents in your knowledge base: 8584\n"
          ]
        }
      ],
      "source": [
        "# rfm_docs, policy_docs, summary_docs\n",
        "\n",
        "all_docs = rfm_docs + policy_docs + summary_docs + [notebook_text]\n",
        "\n",
        "print(f\"Total documents in your knowledge base: {len(all_docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "OJi1iJ4Ikj4Z",
        "outputId": "b5c854ed-3a6d-4df6-edfa-91956a4bf9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-3-pro-preview\n",
            "models/gemini-3-flash-preview\n",
            "models/gemini-3.1-pro-preview\n",
            "models/gemini-3.1-pro-preview-customtools\n",
            "models/gemini-3-pro-image-preview\n",
            "models/nano-banana-pro-preview\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/deep-research-pro-preview-12-2025\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
            "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# Ensure your API key is configured\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# List all available models and print their names\n",
        "for model in genai.list_models():\n",
        "    print(model.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDUXMHSQkj1f",
        "outputId": "68b19433-34e8-4573-a05a-53e2cc0cebbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries for RAG components\n",
        "!pip install -qU chromadb langchain-text-splitters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-03qBU7whwqL"
      },
      "source": [
        "**Structuring Documents for Vector Indexing (RAG Preparation)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "petiOhg3hykB"
      },
      "source": [
        "This code snippet is performing a crucial data preparation step for a Retrieval-Augmented Generation (RAG) system by intelligently structuring documents for vector indexing. It first separates a single, long document (notebook_text, which is the entire content of the Colab notebook) from a list of shorter, specific summary documents (short_docs). It then uses the RecursiveCharacterTextSplitter to break the long notebook text into smaller, overlapping chunks (1000 characters with 200 character overlap) to ensure comprehensive context is preserved across splits. Finally, it combines these newly chunked notebook sections with the original short summary documents (like the RFM and policy recommendation texts) into a unified list called final_documents, ready to be indexed for efficient retrieval by a conversational AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4fgkvjXkjpq",
        "outputId": "b1cb9295-7980-4c6d-f400-13a5c638cb40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents after chunking: 8746\n",
            "Example chunk: ### 9. Future Work\n",
            "\n",
            "*   **Model Retraining:** Regularly retrain the model with fresh data to capture evolving customer behavior.\n",
            "*   **Feature Expansi...\n"
          ]
        }
      ],
      "source": [
        "# Preprocessing and Chunking\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Separate the long notebook text from the rest of the documents\n",
        "notebook_text = all_docs[-1]\n",
        "short_docs = all_docs[:-1]\n",
        "\n",
        "# 1. Chunk the long notebook document\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "chunked_notebook_docs = text_splitter.create_documents([notebook_text])\n",
        "\n",
        "# Convert LangChain Document objects back to simple strings\n",
        "chunked_text_list = [doc.page_content for doc in chunked_notebook_docs]\n",
        "\n",
        "# 2. Combine all documents: short, specific documents + chunked notebook text\n",
        "final_documents = short_docs + chunked_text_list\n",
        "\n",
        "print(f\"Total documents after chunking: {len(final_documents)}\")\n",
        "print(f\"Example chunk: {final_documents[-1][:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y14RZDj-mHfR"
      },
      "source": [
        "**Vectorizing and Indexing Knowledge Base (RAG System Setup)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQlCyFrJiDBy"
      },
      "source": [
        "This code snippet performs the critical task of vectorizing and indexing a list of documents to create a knowledge base for a Retrieval-Augmented Generation (RAG) system. It initializes the Google Gemini client and defines the gemini-embedding-001 model for vector creation. It then sets up a persistent ChromaDB vector store in a local directory. The code proceeds to iterate through the final_documents (the combined, chunked text) in batches. For each batch, it calls the Gemini API to generate high-quality vector embeddings optimized for document retrieval. Finally, it stores these embeddings, along with their original text content and unique IDs, into the ChromaDB collection, thereby making the knowledge base searchable via semantic similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9_Jca1K8mJRr",
        "outputId": "edfa8562-c736-497f-c0ad-10a9b50ca2ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully indexed 8763 documents.\n",
            "Vector Store is ready for Retrieval.\n"
          ]
        }
      ],
      "source": [
        "# Creating and Indexing Embeddings\n",
        "import google.generativeai as genai\n",
        "import chromadb\n",
        "import os\n",
        "\n",
        "# Initialize the Gemini Client (already configured from cell 9)\n",
        "# Ensure os.getenv('GOOGLE_API_KEY') is available.\n",
        "\n",
        "# 1. Define the embedding model\n",
        "EMBEDDING_MODEL = 'models/gemini-embedding-001'\n",
        "\n",
        "# 2. Setup the ChromaDB Client and Collection\n",
        "# This creates a directory 'chroma_db_ecommerce' to store the vector database\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db_ecommerce\")\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"ecommerce_rag_knowledge_base\",\n",
        ")\n",
        "\n",
        "# 3. Create Embeddings in Batches and Store in ChromaDB\n",
        "# For performance, we generate and store embeddings in small batches.\n",
        "batch_size = 100\n",
        "for i in range(0, len(final_documents), batch_size):\n",
        "    batch_docs = final_documents[i:i + batch_size]\n",
        "    batch_ids = [f\"doc_{j}\" for j in range(i, i + len(batch_docs))]\n",
        "\n",
        "    # Generate embeddings using the Gemini API\n",
        "    # We use a list comprehension to handle the response structure\n",
        "    result = genai.embed_content(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        content=batch_docs,\n",
        "        task_type=\"RETRIEVAL_DOCUMENT\" # Optimizes embeddings for RAG retrieval\n",
        "    )\n",
        "    batch_embeddings = result['embedding']\n",
        "\n",
        "    # Add the embeddings and text to ChromaDB\n",
        "    collection.add(\n",
        "        embeddings=batch_embeddings,\n",
        "        documents=batch_docs,\n",
        "        ids=batch_ids\n",
        "    )\n",
        "\n",
        "print(f\"Successfully indexed {collection.count()} documents.\")\n",
        "print(\"Vector Store is ready for Retrieval.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXUOFOWAoAhD"
      },
      "source": [
        "**Implementing the Core RAG Chatbot Query Function**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33C0l8N7iPe7"
      },
      "source": [
        "This code snippet defines and demonstrates the core function of a Retrieval-Augmented Generation (RAG) chatbot designed for E-commerce analytics. The rag_chat_query function takes a user question and performs a two-step process: Retrieval and Generation.\n",
        "\n",
        "Retrieval: It first converts the user's question into a vector embedding using the Gemini API. It then uses this vector to query the pre-built ChromaDB vector store for the top_k most semantically relevant text chunks from the knowledge base, which become the context.\n",
        "\n",
        "Generation: It augments a prompt by combining the retrieved context with the original user question, and applies a system instruction to act as an expert E-commerce chatbot. Finally, it passes this complete prompt to the Gemini-2.5-flash model, which generates a concise answer based only on the provided context, thus ensuring grounded and factual responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "G7JEXMMcmOCF",
        "outputId": "962e87bf-3874-4369-a0f0-e0cc5f602b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Question: What is the recommended retention action and projected ROI for customer 12349, and what is their recency?\n",
            "\n",
            "--- Chatbot Answer ---\n",
            "For customer 12349, the recommended retention action is 'sms' with a projected ROI of $2.99. Information regarding recency is not available in the provided context.\n",
            "\n",
            "--- Context Used for Answer ---\n",
            "For customer 12349, the recommended retention action is to use 'sms'. This action has a projected ROI of $2.99.\n",
            "---\n",
            "For customer 12549, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $7.56.\n",
            "---\n",
            "For customer 17349, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $3.18.\n",
            "---\n",
            "For customer 13249, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $3.87.\n",
            "---\n",
            "For customer 14549, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $4.20.\n"
          ]
        }
      ],
      "source": [
        "# Define the RAG Query Function\n",
        "def rag_chat_query(user_query: str, top_k: int = 3, llm_model: str = 'models/gemini-2.5-flash'):\n",
        "    # 1. Retrieval: Convert query to embedding and search the vector store\n",
        "\n",
        "    # Generate the embedding for the user's query\n",
        "    query_embedding_result = genai.embed_content(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        content=[user_query],\n",
        "        task_type=\"RETRIEVAL_QUERY\" # Optimizes query for retrieval\n",
        "    )\n",
        "    query_embedding = query_embedding_result['embedding'][0]\n",
        "\n",
        "    # Use the vector store to search for similar documents (context)\n",
        "    retrieved_results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=top_k,\n",
        "        include=['documents']\n",
        "    )\n",
        "\n",
        "    # Combine retrieved documents into a single context string\n",
        "    retrieved_context = \"\\n---\\n\".join(retrieved_results['documents'][0])\n",
        "\n",
        "    # 2. Augmentation & Generation: Build the prompt and call the LLM\n",
        "\n",
        "    # Define a system instruction for the LLM\n",
        "    system_instruction = (\n",
        "        \"You are an expert E-commerce Customer Retention and Analytics Chatbot. \"\n",
        "        \"Your task is to answer user questions strictly based on the provided CONTEXT. \"\n",
        "        \"Do not use external knowledge. Be concise and professional.\"\n",
        "    )\n",
        "\n",
        "    # Create the final prompt with the retrieved context\n",
        "    prompt = f\"\"\"\n",
        "    CONTEXT:\n",
        "    ---\n",
        "    {retrieved_context}\n",
        "    ---\n",
        "\n",
        "    QUESTION: {user_query}\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the Gemini LLM to generate the final response\n",
        "    response = genai.GenerativeModel(\n",
        "        model_name=llm_model,\n",
        "        system_instruction=system_instruction\n",
        "    ).generate_content(prompt)\n",
        "\n",
        "    return response.text, retrieved_context\n",
        "\n",
        "# Example Query\n",
        "user_question = \"What is the recommended retention action and projected ROI for customer 12349, and what is their recency?\"\n",
        "answer, context = rag_chat_query(user_question, top_k=5)\n",
        "\n",
        "print(f\"User Question: {user_question}\")\n",
        "print(f\"\\n--- Chatbot Answer ---\\n{answer}\")\n",
        "print(f\"\\n--- Context Used for Answer ---\\n{context}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzKZnSN3otTK"
      },
      "source": [
        "**Merging Customer Profiles with Contextual Policy Recommendations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRTKsEqQou23"
      },
      "source": [
        "This code snippet's primary function is to integrate two separate customer datasets‚Äîone containing Recency, Frequency, and Monetary (RFM) metrics (rfm) and the other containing contextual policy recommendations (context_recon)‚Äîinto a single comprehensive DataFrame. It first attempts to load both dataframes, then renames the customer identifier column in the recommendation data (Customer_ID to Customer ID) for consistency. It then performs a left merge using 'Customer ID' as the key, effectively linking each customer's purchasing behavior (RFM) with their assigned retention action and projected return. Finally, it previews the new combined structure and saves the resulting dataset as merged_customer_profiles.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56mO2cXSmN0D",
        "outputId": "c0327b76-74a8-44a0-a953-9fe970af68a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Merged Data Head ---\n",
            "   Customer ID     LastPurchaseDate  Recency_x  Frequency_x  Monetary_x  \\\n",
            "0        12346  2010-10-04 16:33:00         66           14      372.86   \n",
            "1        12347  2010-12-07 14:57:00          2            2     1323.32   \n",
            "2        12348  2010-09-27 14:59:00         73            1      222.16   \n",
            "3        12349  2010-10-28 08:23:00         42            3     2064.39   \n",
            "4        12351  2010-11-29 15:23:00         10            1      300.93   \n",
            "\n",
            "   Churn_Label Risk_Tier         Country  Recency_y  Frequency_y  Monetary_y  \\\n",
            "0            1    Medium  United Kingdom         66           14      372.86   \n",
            "1            0       NaN         Iceland          2            2     1323.32   \n",
            "2            1       NaN         Finland         73            1      222.16   \n",
            "3            1    Medium           Italy         42            3     2064.39   \n",
            "4            0       NaN     Unspecified         10            1      300.93   \n",
            "\n",
            "  Chosen_Action  Chosen_Score  Estimated_Reward  \n",
            "0   call+coupon    231.325229          2.152547  \n",
            "1         email    793.994494          0.000000  \n",
            "2          none    140.312878          0.000000  \n",
            "3           sms   1358.440668          2.989122  \n",
            "4    sms+coupon    180.661649          0.000000  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Re-load dataframes to ensure they are available\n",
        "try:\n",
        "    rfm = pd.read_csv(\"rfm.xls\")\n",
        "    context_recon = pd.read_csv(\"contextual_policy_recommendations.xls\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading files. Ensure 'rfm.csv' and 'contextual_policy_recommendations.csv' are in the directory: {e}\")\n",
        "    # Handle error or exit\n",
        "\n",
        "# Rename the key column in context_recon to match rfm before merging\n",
        "context_recon = context_recon.rename(columns={'Customer_ID': 'Customer ID'})\n",
        "\n",
        "# Merge the two dataframes on 'Customer ID'\n",
        "merged_customer_data = pd.merge(rfm, context_recon, on='Customer ID', how='left')\n",
        "\n",
        "# Preview the new merged data structure\n",
        "print(\"--- Merged Data Head ---\")\n",
        "print(merged_customer_data.head())\n",
        "\n",
        "# Save the merged DataFrame for inspection (optional)\n",
        "merged_customer_data.to_csv(\"merged_customer_profiles.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwcSzTSdo8MS"
      },
      "source": [
        "**Generating Comprehensive Customer Profile Documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCnF1u22il0c"
      },
      "source": [
        "This code snippet's purpose is to create comprehensive, single-source documents for each customer by combining their behavioral data with their strategic recommendations. It iterates through the merged_customer_data DataFrame, which contains both RFM metrics (Recency, Frequency, Monetary) and policy recommendations (Chosen Action, Estimated Reward). For every row, it extracts all these distinct features and synthesizes them into one rich, natural-language string. These detailed profile strings are collected into the merged_docs list, which are highly informative documents ready for use in advanced downstream systems like a Retrieval-Augmented Generation (RAG) knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbZJWQGsmNwG",
        "outputId": "c7834adf-e004-4643-94a3-0cdac426a1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example of Improved Document for Customer 12349 ---\n",
            "Customer Profile 12349: Recency is 42 days, Frequency is 3 times, and Monetary spend is $2064.39. This customer is classified as churned. The recommended retention action is to use 'sms' with a projected ROI of $2.99.\n"
          ]
        }
      ],
      "source": [
        "# Create new documents from the merged DataFrame\n",
        "merged_docs = []\n",
        "for index, row in merged_customer_data.iterrows():\n",
        "    customer_id = row['Customer ID']\n",
        "    recency = row['Recency_x']\n",
        "    frequency = row['Frequency_x']\n",
        "    monetary = row['Monetary_x']\n",
        "    churn = 'churned' if row['Churn_Label'] == 1 else 'not churned'\n",
        "    action = row['Chosen_Action']\n",
        "    reward = row['Estimated_Reward']\n",
        "\n",
        "    # Combine all information into one single, rich document\n",
        "    text = (\n",
        "        f\"Customer Profile {customer_id}: \"\n",
        "        f\"Recency is {recency} days, Frequency is {frequency} times, \"\n",
        "        f\"and Monetary spend is ${monetary:.2f}. \"\n",
        "        f\"This customer is classified as {churn}. \"\n",
        "        f\"The recommended retention action is to use '{action}' \"\n",
        "        f\"with a projected ROI of ${reward:.2f}.\"\n",
        "    )\n",
        "    merged_docs.append(text)\n",
        "\n",
        "# Example of the improved document for customer 12349\n",
        "# (The index for 12349 is 3 based on the printout from the first notebook cell)\n",
        "print(\"\\n--- Example of Improved Document for Customer 12349 ---\")\n",
        "print(merged_docs[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jFhQBF0tUI9"
      },
      "source": [
        "**Initiating and Running the Interactive RAG Chatbot Session**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OVpJCkrtY5v"
      },
      "source": [
        "This code snippet defines the function start_chatbot_session(), which establishes and runs a continuous, interactive conversational loop for the E-commerce Retrieval-Augmented Generation (RAG) system. The function initializes the user interface, prompting the user for input. Inside a while loop, it accepts a user query, checks for exit commands (quit or exit), and then passes the input to the previously defined rag_chat_query function. This query function retrieves relevant context from the vector database and uses the Gemini LLM to generate an answer. The loop continuously prints the chatbot's response and, for transparency, the source context used to generate that answer, until the user explicitly terminates the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpQaa5oWmNpq"
      },
      "outputs": [],
      "source": [
        "# Define and Run the Continuous Chat Loop\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# NOTE: The rag_chat_query function and the 'collection' object are assumed\n",
        "# to be defined and configured from the previous successful steps.\n",
        "# The EMBEDDING_MODEL is 'models/gemini-embedding-001' and LLM is 'models/gemini-2.5-flash'.\n",
        "\n",
        "def start_chatbot_session():\n",
        "    \"\"\"Initializes and runs the continuous RAG chatbot session.\"\"\"\n",
        "\n",
        "    print(\"--- E-commerce RAG Chatbot Initialized ---\")\n",
        "    print(\"Ask a question about customer data or retention policies.\")\n",
        "    print(\"Type 'quit' or 'exit' to end the session.\\n\")\n",
        "\n",
        "    # Main conversational loop\n",
        "    while True:\n",
        "        # Get user input\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        # Check for exit commands\n",
        "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "            print(\"\\nChatbot session ended. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not user_input.strip():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Call the RAG function (from the previous step)\n",
        "            # We use top_k=1 since the customer data is now fully merged\n",
        "            # For general policy questions, top_k can be higher (e.g., 3)\n",
        "            answer, context = rag_chat_query(user_input, top_k=3)\n",
        "\n",
        "            print(f\"\\nü§ñ Chatbot: {answer}\")\n",
        "\n",
        "            # Optionally, show the source context for verification\n",
        "            print(\"\\n[Source Context Retrieved]:\")\n",
        "            print(context)\n",
        "            print(\"-------------------------------------------\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå An error occurred: {e}. Please try again.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TNa7DI593Y4"
      },
      "source": [
        "**Interactive DashBoard**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the Chatbot\n",
        "start_chatbot_session()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UpEh66gx-q0H",
        "outputId": "743dacca-1bb4-4f05-f711-f9a0ec04672f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- E-commerce RAG Chatbot Initialized ---\n",
            "Ask a question about customer data or retention policies.\n",
            "Type 'quit' or 'exit' to end the session.\n",
            "\n",
            "You: Which customers have the highest Recency scores in the dataset?\n",
            "\n",
            "ü§ñ Chatbot: All listed customers (12481, 17841, 18041) have a Recency score of 0 days.\n",
            "\n",
            "[Source Context Retrieved]:\n",
            "Customer 12481 has a recency of 0 days, a purchase frequency of 7 times, and a total monetary spend of $6171.07. This customer is classified as not churned.\n",
            "---\n",
            "Customer 17841 has a recency of 0 days, a purchase frequency of 123 times, and a total monetary spend of $26664.52. This customer is classified as not churned.\n",
            "---\n",
            "Customer 18041 has a recency of 0 days, a purchase frequency of 21 times, and a total monetary spend of $4520.34. This customer is classified as not churned.\n",
            "-------------------------------------------\n",
            "\n",
            "You: Identify customers with high Monetary value but low Frequency.\n",
            "\n",
            "ü§ñ Chatbot: Customer 15478 has a monetary spend of $583.56 and a purchase frequency of 1.\n",
            "Customer 13559 has a monetary spend of $330.00 and a purchase frequency of 1.\n",
            "Customer 14892 has a monetary spend of $202.35 and a purchase frequency of 1.\n",
            "\n",
            "[Source Context Retrieved]:\n",
            "Customer 14892 has a recency of 325 days, a purchase frequency of 1 times, and a total monetary spend of $202.35. This customer is classified as churned.\n",
            "---\n",
            "Customer 13559 has a recency of 178 days, a purchase frequency of 1 times, and a total monetary spend of $330.00. This customer is classified as churned.\n",
            "---\n",
            "Customer 15478 has a recency of 210 days, a purchase frequency of 1 times, and a total monetary spend of $583.56. This customer is classified as churned.\n",
            "-------------------------------------------\n",
            "\n",
            "You: How many active customers made purchases in the last 30 days?\n",
            "\n",
            "ü§ñ Chatbot: Based on the provided context, there are 3 active customers who made purchases in the last 30 days.\n",
            "\n",
            "[Source Context Retrieved]:\n",
            "Customer 12961 has a recency of 14 days, a purchase frequency of 1 times, and a total monetary spend of $211.99. This customer is classified as not churned.\n",
            "---\n",
            "Customer 13030 has a recency of 3 days, a purchase frequency of 1 times, and a total monetary spend of $402.36. This customer is classified as not churned.\n",
            "---\n",
            "Customer 15961 has a recency of 14 days, a purchase frequency of 1 times, and a total monetary spend of $451.29. This customer is classified as not churned.\n",
            "-------------------------------------------\n",
            "\n",
            "You: Which customers should receive discount-based campaigns?\n",
            "\n",
            "ü§ñ Chatbot: Customers 15633, 12631, and 14861 should receive discount-based campaigns, as the recommended retention action for them is 'sms+coupon'.\n",
            "\n",
            "[Source Context Retrieved]:\n",
            "For customer 15633, the recommended retention action is to use 'sms+coupon'. This action has a projected ROI of $0.77.\n",
            "---\n",
            "For customer 12631, the recommended retention action is to use 'sms+coupon'. This action has a projected ROI of $3.02.\n",
            "---\n",
            "For customer 14861, the recommended retention action is to use 'sms+coupon'. This action has a projected ROI of $3.29.\n",
            "-------------------------------------------\n",
            "\n",
            "You: Which action plan has the highest average ROI?\n",
            "\n",
            "ü§ñ Chatbot: The 'call+coupon' action plan has the highest average ROI of $3.245.\n",
            "\n",
            "[Source Context Retrieved]:\n",
            "For customer 13451, the recommended retention action is to use 'sms+coupon'. This action has a projected ROI of $2.93.\n",
            "---\n",
            "For customer 17541, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $1.92.\n",
            "---\n",
            "For customer 17345, the recommended retention action is to use 'call+coupon'. This action has a projected ROI of $4.57.\n",
            "-------------------------------------------\n",
            "\n",
            "You: What is the RFM summary for Customer 13362, 18272?? Which customers have the highest Recency scores in the dataset? Identify customers with high Monetary value but low Frequency. Identify customers with highest Monetary value but low Frequency. Show me the top 3 most loyal customers based on purchase history. How many active customers made purchases in the last 30 days? What trends can you see in the purchasing behavior of repeat buyers? Which customers are most likely to upgrade to premium offers? Customer Insights & Profiles (all 2671 customers profile queries) Retention Policy & Recommended Actions What retention strategy is recommended for Customer 16660? Which customers should receive discount-based campaigns? Which action plan has the highest average ROI? If I invest $1 in a retention program, what‚Äôs the expected return?\n",
            "\n",
            "ü§ñ Chatbot: Based on the provided context:\n",
            "\n",
            "*   **RFM summary for Customer 13362:** Recency of 13 days, purchase frequency of 5 times, and a total monetary spend of $2303.33. This customer is classified as not churned.\n",
            "*   **RFM summary for Customer 18272:** Recency of 64 days, purchase frequency of 4 times, and a total monetary spend of $1307.40. This customer is classified as churned.\n",
            "*   **Customers with the highest Recency scores:** Customer 18272 has the highest recency score of 64 days.\n",
            "*   **Customers with high Monetary value but low Frequency:** Based on the limited dataset, no customer clearly fits this profile. Customer 13362 has the highest monetary value ($2303.33) but also the highest frequency (5 times). Customer 18262 has the lowest frequency (2 times) but also the lowest monetary value ($428.06).\n",
            "*   **Customers with highest Monetary value but low Frequency:** Similar to the above, no customer in the dataset fits this specific combination.\n",
            "*   **Top 3 most loyal customers based on purchase history:**\n",
            "    1.  Customer 13362 (Frequency: 5, Recency: 13 days)\n",
            "    2.  Customer 18272 (Frequency: 4, Recency: 64 days)\n",
            "    3.  Customer 18262 (Frequency: 2, Recency: 28 days)\n",
            "*   **Active customers who made purchases in the last 30 days:** Two active customers made purchases in the last 30 days: Customer 13362 (13 days) and Customer 18262 (28 days).\n",
            "\n",
            "The provided context does not contain information to answer the following questions:\n",
            "*   What trends can you see in the purchasing behavior of repeat buyers?\n",
            "*   Which customers are most likely to upgrade to premium offers?\n",
            "*   Customer Insights & Profiles (all 2671 customers profile queries)\n",
            "*   Retention Policy & Recommended Actions\n",
            "*   What retention strategy is recommended for Customer 16660?\n",
            "*   Which customers should receive discount-based campaigns?\n",
            "*   Which action plan has the highest average ROI?\n",
            "*   If I invest $1 in a retention program, what‚Äôs the expected return?\n",
            "\n",
            "[Source Context Retrieved]:\n",
            "Customer 13362 has a recency of 13 days, a purchase frequency of 5 times, and a total monetary spend of $2303.33. This customer is classified as not churned.\n",
            "---\n",
            "Customer 18262 has a recency of 28 days, a purchase frequency of 2 times, and a total monetary spend of $428.06. This customer is classified as not churned.\n",
            "---\n",
            "Customer 18272 has a recency of 64 days, a purchase frequency of 4 times, and a total monetary spend of $1307.40. This customer is classified as churned.\n",
            "-------------------------------------------\n",
            "\n",
            "You: If I invest $1 in a retention program, what‚Äôs the expected return?\n",
            "\n",
            "ü§ñ Chatbot: The projected ROI for recommended retention actions varies by customer:\n",
            "*   For customer 13659, the projected ROI is $1.10.\n",
            "*   For customer 14755, the projected ROI is $1.29.\n",
            "*   For customer 15759, the projected ROI is $1.40.\n",
            "\n",
            "[Source Context Retrieved]:\n",
            "For customer 13659, the recommended retention action is to use 'sms+coupon'. This action has a projected ROI of $1.10.\n",
            "---\n",
            "For customer 14755, the recommended retention action is to use 'sms+coupon'. This action has a projected ROI of $1.29.\n",
            "---\n",
            "For customer 15759, the recommended retention action is to use 'sms+coupon'. This action has a projected ROI of $1.40.\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G162lUXd_WkG"
      },
      "source": [
        "Python code for 'dashboard_app.py' (do not run here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFvl_MVv_r4y"
      },
      "source": [
        "Open a .txt file in Desktop and save the code of 'dashboard_app.py' and name the .txt fille accordingly as 'dashboard_app.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9B2-bK2-G3w"
      },
      "source": [
        "**We have to use Powershell 7 for command/prompt**\n",
        "\n",
        "1. copy this part 'pip install streamlit pandas numpy plotly' and press enter\n",
        "2. type 'cd OneDrive' and press enter\n",
        "3. type 'cd Desktop' and press enter\n",
        "4. type 'streamlit run dashboard_app.py' and see the magic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gRJXMkAKp2w"
      },
      "source": [
        "1. $env:GOOGLE_API_KEY = \"your-real-key\"\n",
        "2. python prepare_vector_store.py ...\n",
        "3. streamlit run streamlit_app.py\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JZqw_0gbv-Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4sWnZclzb95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpNYQmtiAFfU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}